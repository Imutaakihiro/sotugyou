import pandas as pd
import MeCab
import neologdn
import demoji

class MorphologicalAnalyzer:
    def __init__(self, dictionary_path=""):
        """
        MeCabã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Args:
            dictionary_path (str, optional): MeCabã®è¾æ›¸ãƒ‘ã‚¹ã€‚
                                            ç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¾æ›¸ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
                                            ä¾‹: "-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd"
        """
        try:
            self.tagger = MeCab.Tagger(dictionary_path)
            self.tagger.parse("") # MeCabã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã¨åˆæœŸåŒ–ç¢ºèª
        except RuntimeError as e:
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¾æ›¸ãƒ‘ã‚¹ã®ç¢ºèªã‚’ä¿ƒã™æƒ…å ±ã‚’è¿½åŠ 
            error_message = f"MeCabã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚mecabrcãƒ•ã‚¡ã‚¤ãƒ«ã‚„è¾æ›¸ãƒ‘ã‚¹ï¼ˆ{dictionary_path if dictionary_path else 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'}ï¼‰ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}"
            if "dictionary_path" in str(e).lower() or "mecabrc" in str(e).lower():
                 error_message += "\nã‚·ã‚¹ãƒ†ãƒ ã«MeCabã¨é©åˆ‡ãªè¾æ›¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ã€ã¾ãŸç’°å¢ƒå¤‰æ•° MECABRC ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            raise RuntimeError(error_message) from e

    def load_csv(self, file_path_or_buffer, encoding='utf-8'):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Pandas DataFrameã¨ã—ã¦è¿”ã—ã¾ã™ã€‚

        Args:
            file_path_or_buffer (str or file-like object): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯ãƒãƒƒãƒ•ã‚¡ã€‚
            encoding (str, optional): ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 'utf-8'ã€‚

        Returns:
            pd.DataFrame: èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã€‚

        Raises:
            FileNotFoundError: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€‚
            Exception: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€‚
        """
        try:
            df = pd.read_csv(file_path_or_buffer, encoding=encoding)
            return df
        except FileNotFoundError:
            raise FileNotFoundError(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path_or_buffer}")
        except Exception as e:
            raise Exception(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    def _preprocess_text(self, text):
        """
        å½¢æ…‹ç´ è§£æã®å‰ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†ã—ã¾ã™ã€‚
        - NEologdã«ã‚ˆã‚‹æ­£è¦åŒ–
        - demojiã«ã‚ˆã‚‹çµµæ–‡å­—ã®å‡¦ç†ï¼ˆèª¬æ˜æ–‡ã«ç½®æ›ï¼‰

        Args:
            text (str): å‰å‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆã€‚

        Returns:
            str: å‰å‡¦ç†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã€‚
        """
        if not isinstance(text, str):
            return "" # æ–‡å­—åˆ—ã§ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™ï¼ˆNaNãªã©ã‚’è€ƒæ…®ï¼‰
        
        text = str(text) # æ˜ç¤ºçš„ã«æ–‡å­—åˆ—ã«å¤‰æ›
        text = neologdn.normalize(text)
        # text = demoji.replace_string(text, " ") # çµµæ–‡å­—ã‚’ã‚³ãƒ­ãƒ³ã§å›²ã¾ã‚ŒãŸåå‰ã«ç½®æ›ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç¬¬äºŒå¼•æ•°ã®æ–‡å­—åˆ—ã€‚
        return text

    def analyze_text(self, text):
        """
        å˜ä¸€ã®ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’å½¢æ…‹ç´ è§£æã—ã€çµæœã‚’ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã—ã¾ã™ã€‚
        å„ã‚¿ãƒ—ãƒ«ã¯ (è¡¨å±¤å½¢, å“è©, å“è©ç´°åˆ†é¡1, å“è©ç´°åˆ†é¡2, å“è©ç´°åˆ†é¡3, æ´»ç”¨å‹, æ´»ç”¨å½¢, åŸå½¢, èª­ã¿, ç™ºéŸ³) ã®å½¢å¼ã§ã™ã€‚
        èª­ã¿ã‚„ç™ºéŸ³ãŒãªã„å ´åˆã¯Noneã¨ãªã‚Šã¾ã™ã€‚

        Args:
            text (str): è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆã€‚

        Returns:
            list[tuple]: å½¢æ…‹ç´ è§£æçµæœã®ãƒªã‚¹ãƒˆã€‚
                         BOS/EOSãƒãƒ¼ãƒ‰ã¯é™¤å¤–ã•ã‚Œã¾ã™ã€‚
                         è§£æå¯¾è±¡ãŒç©ºæ–‡å­—åˆ—ã‚„ç©ºç™½ã®ã¿ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚
        """
        processed_text = self._preprocess_text(text)
        if not processed_text.strip(): # å‰å‡¦ç†å¾Œã€ç©ºã¾ãŸã¯ç©ºç™½ã®ã¿ã«ãªã£ãŸå ´åˆ
            return []

        node = self.tagger.parseToNode(processed_text)
        results = []
        while node:
            if node.surface: # è¡¨å±¤å½¢ãŒå­˜åœ¨ã™ã‚‹ãƒãƒ¼ãƒ‰ã®ã¿ (BOS/EOSãƒãƒ¼ãƒ‰å¯¾ç­–)
                features = node.feature.split(',')
                surface = node.surface
                
                # featuresã®è¦ç´ æ•°ã¯9å€‹ã¨ä»®å®šã™ã‚‹ãŒã€è¾æ›¸ã«ã‚ˆã£ã¦ã¯å°‘ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹
                # (å“è©, å“è©ç´°åˆ†é¡1, å“è©ç´°åˆ†é¡2, å“è©ç´°åˆ†é¡3, æ´»ç”¨å‹, æ´»ç”¨å½¢, åŸå½¢, èª­ã¿, ç™ºéŸ³)
                # èª­ã¿(features[7])ã¨ç™ºéŸ³(features[8])ã¯å­˜åœ¨ã—ãªã„å ´åˆ '*' ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§Noneã«å¤‰æ›
                
                token_info = [surface] + features[:7] # è¡¨å±¤å½¢ + 7ã¤ã®ç´ æ€§
                
                # èª­ã¿ã¨ç™ºéŸ³ã®å‡¦ç†
                if len(features) > 7 and features[7] != '*':
                    token_info.append(features[7])
                else:
                    token_info.append(None) # èª­ã¿ãŒãªã„å ´åˆ
                
                if len(features) > 8 and features[8] != '*':
                    token_info.append(features[8])
                else:
                    token_info.append(None) # ç™ºéŸ³ãŒãªã„å ´åˆ
                
                # ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ãŒã‚ã‚Œã°Noneã§åŸ‹ã‚ã‚‹ (æœ€å¤§9å€‹ã®ç‰¹å¾´é‡ + è¡¨å±¤å½¢ = 10è¦ç´ )
                while len(token_info) < 10:
                    token_info.append(None)

                results.append(tuple(token_info))
            node = node.next
        return results

    def analyze_column(self, df, column_name):
        """
        DataFrameã®æŒ‡å®šã•ã‚ŒãŸåˆ—ã«å«ã¾ã‚Œã‚‹å„ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æã—ã¾ã™ã€‚

        Args:
            df (pd.DataFrame): å¯¾è±¡ã®DataFrameã€‚
            column_name (str): å½¢æ…‹ç´ è§£æã‚’è¡Œã„ãŸã„åˆ—ã®åå‰ã€‚

        Returns:
            list[list[tuple]]: DataFrameã®å„è¡Œã«å¯¾ã™ã‚‹å½¢æ…‹ç´ è§£æçµæœã®ãƒªã‚¹ãƒˆã€‚
                               å„å†…éƒ¨ãƒªã‚¹ãƒˆã¯ analyze_text ã®è¿”ã‚Šå€¤ã¨åŒã˜å½¢å¼ã§ã™ã€‚

        Raises:
            ValueError: æŒ‡å®šã•ã‚ŒãŸåˆ—åãŒDataFrameã«å­˜åœ¨ã—ãªã„å ´åˆã€‚
        """
        if column_name not in df.columns:
            raise ValueError(f"æŒ‡å®šã•ã‚ŒãŸåˆ—å '{column_name}' ã¯DataFrameã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—: {df.columns.tolist()}")

        all_analysis_results = []
        for text_content in df[column_name]:
            # Pandasã®æ¬ æå€¤ (NaN) ã‚„ãã®ä»–ã®éæ–‡å­—åˆ—å‹ã‚’å®‰å…¨ã«å‡¦ç†
            analysis_result = self.analyze_text(str(text_content) if pd.notna(text_content) else "")
            all_analysis_results.append(analysis_result)
        return all_analysis_results

if __name__ == '__main__':
    # --- ã“ã®ã‚¯ãƒ©ã‚¹ã®ç°¡å˜ãªä½¿ç”¨ä¾‹ ---
    print("MorphologicalAnalyzerã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")

    # MeCabã®è¾æ›¸ãƒ‘ã‚¹ (ä¾‹: NEologd)
    # ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚ç©ºæ–‡å­—åˆ—ã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¾æ›¸ãŒè©¦ã¿ã‚‰ã‚Œã¾ã™ã€‚
    # ä¾‹: dictionary_arg = "-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd"
    dictionary_arg = "" # ã“ã“ã‚’ç·¨é›†ã—ã¦ç‰¹å®šã®è¾æ›¸ã‚’æŒ‡å®šã§ãã¾ã™

    try:
        analyzer = MorphologicalAnalyzer(dictionary_path=dictionary_arg)
        print(f"MeCabã®åˆæœŸåŒ–æˆåŠŸã€‚è¾æ›¸: {'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ' if not dictionary_arg else dictionary_arg}")
    except RuntimeError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: MeCabã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        print(e)
        print("ãƒ†ã‚¹ãƒˆã‚’ä¸­æ–­ã—ã¾ã™ã€‚MeCabã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ã¨è¾æ›¸ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        exit()

    # 1. ãƒ€ãƒŸãƒ¼ã®CSVãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    data = {
        'ID': [1, 2, 3, 4],
        'Tweet': [
            "ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡ğŸ‘ã€‚ç¾å‘³ã—ã„ã‚ˆã­ã€‚",
            "PythonğŸã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’ã™ã‚‹ã®ã¯æ¥½ã—ã„ã§ã™ï¼ #ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°",
            "ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­â˜€ãŠæ•£æ­©ã«è¡Œã“ã†ã‹ãªï¼Ÿ",
            None # æ¬ æãƒ‡ãƒ¼ã‚¿ã®ãƒ†ã‚¹ãƒˆ
        ],
        'Category': ['æœç‰©', 'æŠ€è¡“', 'æ—¥å¸¸', 'ä¸æ˜']
    }
    sample_df = pd.DataFrame(data)

    print("\n--- ãƒ€ãƒŸãƒ¼DataFrame ---")
    print(sample_df)

    target_column = 'Tweet'
    print(f"\n--- '{target_column}' åˆ—ã®å½¢æ…‹ç´ è§£æçµæœ ---")

    try:
        analysis_results_for_column = analyzer.analyze_column(sample_df, target_column)
        for i, (original_text, result_list) in enumerate(zip(sample_df[target_column], analysis_results_for_column)):
            print(f"\nå…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ {i+1}: {original_text}")
            if not result_list:
                print("  (è§£æçµæœãªã— or ç©ºç™½ãƒ†ã‚­ã‚¹ãƒˆ)")
                continue
            for token in result_list:
                # (è¡¨å±¤å½¢, å“è©, å“è©ç´°åˆ†é¡1, å“è©ç´°åˆ†é¡2, å“è©ç´°åˆ†é¡3, æ´»ç”¨å‹, æ´»ç”¨å½¢, åŸå½¢, èª­ã¿, ç™ºéŸ³)
                print(f"  å½¢æ…‹ç´ : {token[0]:<10} å“è©: {token[1]:<6} åŸå½¢: {token[7] if token[7] else '-':<8} èª­ã¿: {token[8] if token[8] else '-'}")

    except ValueError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        print(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # 2. å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®è§£æãƒ†ã‚¹ãƒˆ
    print("\n--- å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®è§£æãƒ†ã‚¹ãƒˆ ---")
    test_texts = [
        "ã“ã‚Œã¯ãƒšãƒ³ã§ã™ã€‚",
        "ãã‚ƒã‚Šãƒ¼ã±ã¿ã‚…ã±ã¿ã‚…ã¯æ­Œæ‰‹ã§ã™ã€‚",
        "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚",
        "We are testing Morphological Analyzer with English words. MeCab may not be optimal for this."
    ]
    for text in test_texts:
        print(f"\nè§£æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        single_text_result = analyzer.analyze_text(text)
        if not single_text_result:
            print("  (è§£æçµæœãªã— or ç©ºç™½ãƒ†ã‚­ã‚¹ãƒˆ)")
            continue
        for token in single_text_result:
            print(f"  å½¢æ…‹ç´ : {token[0]:<10} å“è©: {token[1]:<6} åŸå½¢: {token[7] if token[7] else '-':<8} èª­ã¿: {token[8] if token[8] else '-'}")

    print("\n--- ãƒ†ã‚¹ãƒˆå®Œäº† ---") 